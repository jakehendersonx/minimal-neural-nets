{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<PAD>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m idx_to_char \u001b[38;5;241m=\u001b[39m {idx: char \u001b[38;5;28;01mfor\u001b[39;00m char, idx \u001b[38;5;129;01min\u001b[39;00m char_to_idx\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Verify PAD token is in the vocabulary\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPAD token index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mchar_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPAD\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Define the LSTM model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWordReverser\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mKeyError\u001b[0m: '<PAD>'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Read words from files\n",
    "with open('words.txt', 'r') as f:\n",
    "    words = [line.strip() for line in f]\n",
    "with open('words-reversed.txt', 'r') as f:\n",
    "    reversed_words = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "# Create vocabulary\n",
    "# Create vocabulary\n",
    "PAD = '<PAD>'\n",
    "vocab = set(PAD)  # Start with PAD token\n",
    "for word in words + reversed_words:\n",
    "    vocab.update(word)\n",
    "vocab = sorted(list(vocab))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "# Verify PAD token is in the vocabulary\n",
    "print(f\"PAD token index: {char_to_idx[PAD]}\")\n",
    "\n",
    "# Define the LSTM model\n",
    "class WordReverser(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(WordReverser, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
    "                torch.zeros(1, batch_size, self.hidden_size))\n",
    "\n",
    "# Create a custom dataset\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, words, reverse_words, char_to_idx, max_len):\n",
    "        self.words = words\n",
    "        self.reverse_words = reverse_words\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = list(self.words[idx])\n",
    "        reverse_word = list(self.reverse_words[idx])\n",
    "        word_encoded = [char_to_idx[c] for c in word]\n",
    "        reverse_encoded = [char_to_idx[c] for c in reverse_word]\n",
    "        \n",
    "        # Pad sequences\n",
    "        word_encoded += [char_to_idx[PAD]] * (self.max_len - len(word_encoded))\n",
    "        reverse_encoded += [char_to_idx[PAD]] * (self.max_len - len(reverse_encoded))\n",
    "        \n",
    "        return (torch.tensor(word_encoded), torch.tensor(reverse_encoded))\n",
    "\n",
    "# Prepare the data\n",
    "max_len = max(max(len(w) for w in words), max(len(w) for w in reversed_words)) + 2  # +2 for SOS and EOS\n",
    "dataset = WordDataset(words, reversed_words, char_to_idx, max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = len(char_to_idx)\n",
    "hidden_size = 256\n",
    "output_size = len(char_to_idx)\n",
    "model = WordReverser(input_size, hidden_size, output_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx[PAD])\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            batch_size = input_seq.size(0)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            hidden = tuple(h.to(device) for h in hidden)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(input_seq, hidden)\n",
    "            \n",
    "            # Adjust output and target shapes\n",
    "            output = output.view(-1, output_size)\n",
    "            target = target_seq.view(-1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader, n_epochs=50)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'word_reverser_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_word(model, word, char_to_idx, idx_to_char):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_seq = torch.tensor([[char_to_idx.get(c, char_to_idx[PAD]) for c in word]])\n",
    "        input_seq = input_seq.to(device)\n",
    "        hidden = model.init_hidden(1)\n",
    "        hidden = tuple(h.to(device) for h in hidden)\n",
    "        \n",
    "        output_word = []\n",
    "        for _ in range(len(word)):\n",
    "            output, hidden = model(input_seq, hidden)\n",
    "            top_char_idx = output[0, -1].argmax().item()\n",
    "            top_char = idx_to_char[top_char_idx]\n",
    "            if top_char == PAD:\n",
    "                break\n",
    "            output_word.append(top_char)\n",
    "            input_seq = torch.tensor([[top_char_idx]]).to(device)\n",
    "        \n",
    "        return ''.join(output_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original word: hello\n",
      "Step 0: Predicted <EOS> (index 0)\n",
      "Reversed word: \n",
      "\n",
      "Original word: world\n",
      "Step 0: Predicted <EOS> (index 0)\n",
      "Reversed word: \n",
      "\n",
      "Original word: python\n",
      "Step 0: Predicted <EOS> (index 0)\n",
      "Reversed word: \n",
      "\n",
      "Original word: programming\n",
      "Step 0: Predicted <EOS> (index 0)\n",
      "Reversed word: \n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_words = [\"hello\", \"world\", \"python\", \"programming\"]\n",
    "for test_word in test_words:\n",
    "    print(f\"\\nOriginal word: {test_word}\")\n",
    "    reversed_word = reverse_word(model, test_word, char_to_idx, idx_to_char)\n",
    "    print(f\"Reversed word: {reversed_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the data and create vocabulary\n",
    "with open('words.txt', 'r') as f:\n",
    "    words = [line.strip() for line in f]\n",
    "\n",
    "with open('words-reversed.txt', 'r') as f:\n",
    "    reversed_words = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "SOS = '<SOS>'\n",
    "EOS = '<EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocab = [UNK, PAD, SOS, EOS] + list(set(''.join(words)))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a custom Dataset\n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, words, reversed_words, char_to_idx):\n",
    "        self.words = words\n",
    "        self.reversed_words = reversed_words\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = [SOS] + list(self.words[idx]) + [EOS]\n",
    "        reversed_word = [SOS] + list(self.reversed_words[idx]) + [EOS]\n",
    "        \n",
    "        return (torch.tensor([self.char_to_idx[c] for c in word]),\n",
    "                torch.tensor([self.char_to_idx[c] for c in reversed_word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create DataLoader\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=char_to_idx[PAD], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=char_to_idx[PAD], batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordDataset(words, reversed_words, char_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the model architecture\n",
    "class Seq2SeqModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        embedded_src = self.embedding(src)\n",
    "        _, (hidden, cell) = self.encoder(embedded_src)\n",
    "\n",
    "        embedded_tgt = self.embedding(tgt[:, :-1])  # exclude last token\n",
    "        output, _ = self.decoder(embedded_tgt, (hidden, cell))\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.0529\n",
      "Epoch 2/10, Loss: 0.4642\n",
      "Epoch 3/10, Loss: 0.1189\n",
      "Epoch 4/10, Loss: 0.0503\n",
      "Epoch 5/10, Loss: 0.0296\n",
      "Epoch 6/10, Loss: 0.0232\n",
      "Epoch 7/10, Loss: 0.0183\n",
      "Epoch 8/10, Loss: 0.0145\n",
      "Epoch 9/10, Loss: 0.0146\n",
      "Epoch 10/10, Loss: 0.0129\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "model = Seq2SeqModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "\n",
    "# Step 5: Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_idx[PAD])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Step 6: Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_to_idx: {'<UNK>': 0, '<PAD>': 1, '<SOS>': 2, '<EOS>': 3, 'd': 4, 'l': 5, 'y': 6, 't': 7, 'u': 8, 'p': 9, 'n': 10, 'a': 11, 'f': 12, 'b': 13, 'z': 14, 'g': 15, 'o': 16, 'j': 17, 'i': 18, 'm': 19, 'e': 20, 'w': 21, 'x': 22, 'c': 23, 'h': 24, 'r': 25, 'q': 26, 'v': 27, 'k': 28, 's': 29}\n",
      "idx_to_char: {0: '<UNK>', 1: '<PAD>', 2: '<SOS>', 3: '<EOS>', 4: 'd', 5: 'l', 6: 'y', 7: 't', 8: 'u', 9: 'p', 10: 'n', 11: 'a', 12: 'f', 13: 'b', 14: 'z', 15: 'g', 16: 'o', 17: 'j', 18: 'i', 19: 'm', 20: 'e', 21: 'w', 22: 'x', 23: 'c', 24: 'h', 25: 'r', 26: 'q', 27: 'v', 28: 'k', 29: 's'}\n",
      "Input sequence: tensor([[ 2, 11, 13, 23,  3]])\n",
      "Raw output shape: torch.Size([1, 4, 30])\n",
      "Raw output: tensor([[[-3.0985e+00, -3.1494e+00, -2.9176e+00, -3.2301e+00, -1.2847e+00,\n",
      "          -1.7373e-03, -5.9593e+00,  2.5254e+00, -5.4848e+00,  4.0350e+00,\n",
      "           2.3974e+00, -1.4774e+00,  3.9303e+00, -1.5242e+00,  3.0622e+00,\n",
      "          -2.9223e+00, -3.7550e+00, -2.3512e+00, -8.3012e-01, -4.8540e+00,\n",
      "          -3.2814e+00, -2.1781e+00,  2.8898e+00,  1.2620e+01,  5.0164e+00,\n",
      "          -1.7908e+00,  3.7055e+00,  3.9469e+00,  2.1084e+00,  2.2806e+00],\n",
      "         [-3.3310e+00, -3.8279e+00, -3.4103e+00, -2.9893e+00, -2.4352e+00,\n",
      "           7.9013e-01, -7.0080e+00,  4.8501e+00, -6.7107e+00,  6.1520e+00,\n",
      "           2.3118e+00, -1.1058e+01,  6.7932e+00, -4.1500e-01,  2.7816e+00,\n",
      "          -1.4690e+00, -4.4159e+00,  1.9609e+00, -6.6557e-01, -2.8021e+00,\n",
      "          -4.4679e+00, -1.5603e+00,  3.4030e+00,  1.2479e+01,  8.0674e+00,\n",
      "          -1.8207e-01,  3.9948e+00,  4.5053e+00,  1.7733e+00,  3.4957e+00],\n",
      "         [-3.5384e+00, -3.9602e+00, -3.5783e+00, -8.1347e-01, -2.8062e+00,\n",
      "           1.0620e+00, -7.7648e+00, -5.2071e-02, -7.6521e+00,  1.9029e+00,\n",
      "           2.8460e+00, -7.9127e+00,  2.5563e+00, -7.9020e+00, -9.6094e-01,\n",
      "          -5.1885e+00, -6.0881e+00, -2.0752e+00,  2.9090e+00, -8.5170e-01,\n",
      "           1.2912e+00, -4.9084e+00,  7.0553e+00,  1.4452e+01,  3.7362e+00,\n",
      "           4.5470e+00,  2.2780e+00,  3.4012e+00, -9.2609e-01,  4.2072e+00],\n",
      "         [-2.3126e+00, -1.6496e+00, -2.2329e+00,  9.1400e+00, -2.5238e+00,\n",
      "          -1.9659e+00, -4.1254e+00, -3.0060e+00, -3.9192e+00, -8.0630e-01,\n",
      "           4.1325e-01, -8.5039e+00, -3.1248e-01, -2.5712e+00, -4.4502e+00,\n",
      "          -3.4369e+00, -3.2850e+00,  2.9354e+00,  5.1608e+00,  1.1819e+00,\n",
      "           5.7643e+00, -3.5220e+00,  3.7751e+00,  1.7829e+00, -1.3743e+00,\n",
      "           2.0283e+00,  2.0342e-01,  3.4177e-02, -3.9086e+00,  1.6182e+00]]])\n",
      "Predicted indices: tensor([[23, 23, 23,  3]])\n",
      "Index 23 maps to character 'c'\n",
      "Index 23 maps to character 'c'\n",
      "Index 3 maps to character '<EOS>'\n",
      "Original word: abc\n",
      "Reversed word: cc\n"
     ]
    }
   ],
   "source": [
    "def reverse_word(model, word, char_to_idx, idx_to_char):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare input sequence\n",
    "        input_seq = torch.tensor([[char_to_idx.get(c, char_to_idx[UNK]) for c in [SOS] + list(word) + [EOS]]])\n",
    "        input_seq = input_seq.to(device)\n",
    "        print(\"Input sequence:\", input_seq)\n",
    "        \n",
    "        # Run inference\n",
    "        output = model(input_seq, input_seq)\n",
    "        print(\"Raw output shape:\", output.shape)\n",
    "        print(\"Raw output:\", output)\n",
    "        \n",
    "        # Get the most likely character at each position\n",
    "        _, predicted = output.max(2)\n",
    "        print(\"Predicted indices:\", predicted)\n",
    "        \n",
    "        # Convert to characters, ignoring SOS and stopping at EOS\n",
    "        reversed_word = []\n",
    "        for char_idx in predicted[0][1:]:  # Start from index 1 to skip SOS\n",
    "            char = idx_to_char[char_idx.item()]\n",
    "            print(f\"Index {char_idx.item()} maps to character '{char}'\")\n",
    "            if char == EOS:\n",
    "                break\n",
    "            reversed_word.append(char)\n",
    "        return ''.join(reversed_word)\n",
    "\n",
    "# Print dictionaries\n",
    "print(\"char_to_idx:\", char_to_idx)\n",
    "print(\"idx_to_char:\", idx_to_char)\n",
    "\n",
    "# Test the model\n",
    "test_word = \"abc\"\n",
    "reversed_word = reverse_word(model, test_word, char_to_idx, idx_to_char)\n",
    "print(f\"Original word: {test_word}\")\n",
    "print(f\"Reversed word: {reversed_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: tensor([[ 2, 11, 13, 23,  4, 20, 12,  3]])\n",
      "Raw output shape: torch.Size([1, 7, 30])\n",
      "Raw output: tensor([[[-2.5803e+00, -2.2422e+00, -3.2119e+00, -2.8323e+00,  7.9035e+00,\n",
      "          -9.7744e-01, -2.4823e+00,  5.8987e+00,  1.0777e+00,  1.9785e+00,\n",
      "          -4.1541e+00, -3.4420e+00,  8.7895e+00, -1.6218e-01,  6.2237e-01,\n",
      "           4.7465e+00, -2.0778e+00,  2.3103e+00, -5.7122e+00, -1.0499e+00,\n",
      "           3.0753e+00,  3.4503e+00,  7.6304e-01, -8.0320e-01, -4.2621e+00,\n",
      "           1.4373e+00,  2.5141e-01, -6.5432e-01,  1.4647e+00, -7.2393e+00],\n",
      "         [-3.5698e+00, -3.2880e+00, -4.0060e+00, -4.2419e+00,  5.3760e+00,\n",
      "           1.0697e+00, -4.5486e+00,  6.1545e+00, -2.4493e-01,  3.8938e+00,\n",
      "          -3.5216e+00, -1.1525e+01,  1.0733e+01, -2.9845e+00, -9.2221e-01,\n",
      "           3.0360e+00, -2.8825e+00,  4.7221e+00, -4.1529e+00, -2.0185e+00,\n",
      "           5.7998e+00,  2.7859e+00,  1.9485e+00,  2.8304e+00, -1.7282e+00,\n",
      "           2.4965e+00, -8.3607e-01,  1.6259e+00,  1.1902e+00, -4.7467e+00],\n",
      "         [-3.8590e+00, -3.5425e+00, -4.0027e+00, -4.9423e+00,  3.5540e+00,\n",
      "           1.4522e+00, -3.6901e+00,  3.1336e+00, -2.3088e+00,  3.4337e+00,\n",
      "          -1.3554e+00, -1.0534e+01,  6.6860e+00, -1.0546e+01, -4.1900e+00,\n",
      "          -1.3218e+00, -4.3046e+00,  1.8742e+00, -2.0793e+00, -3.0592e+00,\n",
      "           1.1113e+01, -8.1300e-01,  5.5556e+00,  7.4537e+00, -1.1665e+00,\n",
      "           1.8997e+00, -2.1533e+00,  1.4562e-02,  2.6616e+00,  1.6786e-01],\n",
      "         [-3.8377e+00, -3.3817e+00, -4.0233e+00, -4.0764e+00,  5.2104e+00,\n",
      "           2.3860e+00, -1.2138e-02,  3.4770e-01, -4.4450e-01,  2.7062e-01,\n",
      "          -2.1616e+00, -6.0265e+00,  4.2425e+00, -8.7452e+00, -5.7811e+00,\n",
      "          -2.3506e+00, -3.6074e+00,  1.3668e+00, -1.5711e-02, -2.7139e+00,\n",
      "           1.5822e+01, -1.4774e+00,  4.8909e+00,  1.2089e+00, -5.2091e+00,\n",
      "           2.0191e-01, -4.4856e+00, -9.2764e-01,  2.0416e+00, -1.2038e+00],\n",
      "         [-3.1812e+00, -3.1825e+00, -3.3875e+00, -3.4190e+00, -1.8148e+00,\n",
      "           1.6682e+00, -3.3811e+00, -3.5583e+00,  1.3617e+00, -9.7981e-01,\n",
      "           1.6815e+00, -2.0002e+00,  3.1751e+00, -1.0582e+01, -5.7904e+00,\n",
      "          -5.8006e+00, -7.1398e-01, -1.9182e+00,  4.5380e+00, -4.1962e+00,\n",
      "           2.0351e+01, -4.9150e+00,  4.4267e+00,  3.2030e+00, -5.6788e+00,\n",
      "           7.8252e-01, -4.4249e+00, -5.8627e-01, -3.2927e+00, -3.6284e+00],\n",
      "         [-3.5751e+00, -3.3668e+00, -3.5402e+00, -1.0294e+00, -4.5863e-01,\n",
      "           5.2254e+00, -5.2800e+00,  2.0164e+00, -6.2642e+00,  5.7832e+00,\n",
      "           7.8020e-01, -8.7624e+00,  7.4680e+00, -2.6481e+00, -1.0917e+00,\n",
      "          -4.1605e+00, -4.2105e+00,  1.9004e+00, -3.0477e+00, -6.0469e-01,\n",
      "           1.7823e+00,  8.1936e-01,  3.6352e+00,  7.4226e+00,  6.6485e+00,\n",
      "           5.7353e-01,  3.7889e-01,  2.6104e+00,  7.0645e-01, -1.6846e+00],\n",
      "         [-3.0695e+00, -2.5139e+00, -2.6551e+00,  1.2194e+01, -2.6403e+00,\n",
      "           2.1439e-01, -5.5852e+00, -3.0266e+00, -5.5545e+00,  3.3716e+00,\n",
      "           3.2775e-01, -7.7388e+00,  2.5494e-01, -2.6649e+00, -5.7484e+00,\n",
      "          -6.1439e+00, -3.8858e+00,  1.0171e+00,  1.0147e+00,  1.2575e+00,\n",
      "           1.4414e+00, -1.7346e+00,  3.3140e+00,  4.8809e+00,  2.2032e+00,\n",
      "          -1.3192e-01,  1.7489e-01, -6.3495e-01, -3.5344e+00,  4.5490e+00]]])\n",
      "Predicted indices: tensor([[12, 12, 20, 20, 20, 12,  3]])\n",
      "Index 12 maps to character 'f'\n",
      "Index 20 maps to character 'e'\n",
      "Index 20 maps to character 'e'\n",
      "Index 20 maps to character 'e'\n",
      "Index 12 maps to character 'f'\n",
      "Index 3 maps to character '<EOS>'\n",
      "Original word: abcdef\n",
      "Reversed word: feeef\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_word = \"abcdef\"\n",
    "reversed_word = reverse_word(model, test_word, char_to_idx, idx_to_char)\n",
    "print(f\"Original word: {test_word}\")\n",
    "print(f\"Reversed word: {reversed_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
