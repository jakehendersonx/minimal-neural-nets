{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3400421738624573\n",
      "Epoch 100, Loss: 0.23187130689620972\n",
      "Epoch 200, Loss: 0.15325528383255005\n",
      "Epoch 300, Loss: 0.10130085796117783\n",
      "Epoch 400, Loss: 0.0669640377163887\n",
      "Epoch 500, Loss: 0.044269390404224396\n",
      "Epoch 600, Loss: 0.029268592596054077\n",
      "Epoch 700, Loss: 0.019352572038769722\n",
      "Epoch 800, Loss: 0.01279726717621088\n",
      "Epoch 900, Loss: 0.008463330566883087\n",
      "Input: tensor([[10., 20.],\n",
      "        [15., 25.]])\n",
      "Predicted Sum: tensor([[28.4590],\n",
      "        [38.0539]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network\n",
    "class AddNumbersNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AddNumbersNet, self).__init__()\n",
    "        self.fc = nn.Linear(2, 1)  # Two inputs, one output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Proper weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)  # Xavier initialization for stability\n",
    "        nn.init.zeros_(m.bias)  # Initialize biases to zero\n",
    "\n",
    "# Generate normalized dataset\n",
    "def generate_data(num_samples=1000):\n",
    "    x = torch.rand((num_samples, 2)) * 100  # Random numbers between 0 and 100\n",
    "    x = (x - x.mean()) / x.std()  # Standardize inputs to zero mean and unit variance\n",
    "    y = x.sum(dim=1, keepdim=True)  # Sum the two numbers\n",
    "    return x, y\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = AddNumbersNet()\n",
    "model.apply(init_weights)  # Apply weight initialization\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-4)  # Smaller learning rate and L2 regularization\n",
    "\n",
    "# Training loop\n",
    "x_train, y_train = generate_data(1000)\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(x_train)\n",
    "    loss = criterion(predictions, y_train)\n",
    "\n",
    "    # Check for NaN loss\n",
    "    if torch.isnan(loss).any():\n",
    "        print(f\"Loss is NaN at epoch {epoch}!\")\n",
    "        break\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test the model\n",
    "x_test = torch.tensor([[10.0, 20.0], [15.0, 25.0]])\n",
    "x_test = (x_test - x_train.mean()) / x_train.std()  # Normalize test data\n",
    "y_test = model(x_test)\n",
    "print(\"Input:\", x_test)\n",
    "print(\"Predicted Sum:\", y_test.detach())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
