{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt to ChatGPT-4o:\n",
    "Write a neural network in PyTorch that can add two numbers. \n",
    "Call it AddNet. \n",
    "It should add two whole numbers that are between 0 and 100.\n",
    "Goal 1 is to get it to add two arbitrary whole numbers in a defined range with the lowest possible error.Goal 2 is to keep the neural net and training definitions to be as simple as possible while still accomplishing Goal 1.\n",
    "At each relevant step of your network definition and training, write about why you made the choice to use that \"technique\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500], Loss: 143.0241\n",
      "Epoch [100/500], Loss: 8.8742\n",
      "Epoch [150/500], Loss: 0.6528\n",
      "Epoch [200/500], Loss: 0.4755\n",
      "Epoch [250/500], Loss: 0.3702\n",
      "Epoch [300/500], Loss: 0.3032\n",
      "Epoch [350/500], Loss: 0.2593\n",
      "Epoch [400/500], Loss: 0.2286\n",
      "Epoch [450/500], Loss: 0.2055\n",
      "Epoch [500/500], Loss: 0.1877\n",
      "Test Loss: 0.1801\n",
      "Prediction for input [[23.0, 45.0]]: 68.49 (Expected: 68.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of input numbers\n",
    "NUM_RANGE = 101  # From 0 to 100 inclusive\n",
    "\n",
    "# Define the AddNet model\n",
    "class AddNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AddNet, self).__init__()\n",
    "        # Simple feedforward network with 1 hidden layer\n",
    "        self.fc1 = nn.Linear(2, 10)  # 2 inputs (two numbers) -> 10 hidden neurons\n",
    "        self.fc2 = nn.Linear(10, 1)  # 10 hidden neurons -> 1 output (sum)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation for non-linearity\n",
    "        x = self.fc2(x)  # No activation for output layer (regression task)\n",
    "        return x\n",
    "\n",
    "# Step 1: Initialize the model, loss function, and optimizer\n",
    "model = AddNet()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer for fast convergence\n",
    "\n",
    "# Explain choice of architecture:\n",
    "# - Input: Two numbers as a 2D tensor (e.g., [45, 30])\n",
    "# - Hidden layer: A small fully connected layer with 10 neurons balances simplicity and capacity\n",
    "# - ReLU: Adds non-linearity to allow the network to learn complex relationships\n",
    "# - Output: Single value (sum), no activation because this is a regression problem\n",
    "\n",
    "# Step 2: Generate training data\n",
    "def generate_data(size):\n",
    "    x_data = np.random.randint(0, NUM_RANGE, size=(size, 2))  # Random pairs of integers\n",
    "    y_data = np.sum(x_data, axis=1, keepdims=True)  # Compute sums\n",
    "    return torch.tensor(x_data, dtype=torch.float32), torch.tensor(y_data, dtype=torch.float32)\n",
    "\n",
    "train_x, train_y = generate_data(10000)  # Generate 10,000 training examples\n",
    "\n",
    "# Step 3: Train the model\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    predictions = model(train_x)  # Forward pass\n",
    "    loss = criterion(predictions, train_y)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Explain training process:\n",
    "# - Loss: MSE is suitable for regression because it penalizes large errors\n",
    "# - Optimizer: Adam is chosen for its adaptive learning rate and robustness\n",
    "# - Epochs: 500 iterations are enough to converge for this simple task\n",
    "# - Data: 10,000 examples provide sufficient coverage of input space (0 to 100)\n",
    "\n",
    "# Step 4: Test the model\n",
    "test_x, test_y = generate_data(1000)  # Generate test data\n",
    "with torch.no_grad():  # No gradient computation during testing\n",
    "    test_predictions = model(test_x)\n",
    "    test_loss = criterion(test_predictions, test_y)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# Step 5: Demonstrate prediction\n",
    "example = torch.tensor([[23, 45]], dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    prediction = model(example).item()\n",
    "    print(f\"Prediction for input {example.tolist()}: {prediction:.2f} (Expected: {sum(example.tolist()[0])})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
